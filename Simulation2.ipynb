{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation 2\n",
    "\n",
    "\n",
    "## Installing the OpenAI gym environment\n",
    "\n",
    "In order to run this simulation it is necessary to install the environment we used in *Simulation1* as a *OpenAI gym* environment. To do so, download the environment from the repository [https://github.com/avalds/gym-CTF-SQL](https://github.com/avalds/gym-CTF-SQL):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!git clone https://github.com/avalds/gym-CTF-SQL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then install the repository to make it available to *OpenAI gym*:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -e gym-CTF-SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "We import basic libraries and modules for our simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ctfsql\n",
    "import datetime\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import DQN\n",
    "import evaluate as ev\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "We set up our environment and instantiate an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:57: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:66: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py:122: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:107: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:108: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ctfsql-v0')\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "\n",
    "We train the agent over $10^4$ episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fmzennaro/miniconda2_1/envs/quantumgymstable/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:187: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -42.4    |\n",
      "| steps                   | 5289     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | -32.5    |\n",
      "| steps                   | 9635     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -21.3    |\n",
      "| steps                   | 12865    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -20.3    |\n",
      "| steps                   | 15993    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 81       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -14.3    |\n",
      "| steps                   | 18528    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 79       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -14.2    |\n",
      "| steps                   | 21048    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -15.3    |\n",
      "| steps                   | 23678    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 74       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -12.5    |\n",
      "| steps                   | 26031    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 72       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | -10      |\n",
      "| steps                   | 28132    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 69       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | -14.2    |\n",
      "| steps                   | 30655    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 67       |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | -10.1    |\n",
      "| steps                   | 32769    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 65       |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | -9       |\n",
      "| steps                   | 34766    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 64       |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | -7.1     |\n",
      "| steps                   | 36578    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 62       |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | -7.9     |\n",
      "| steps                   | 38472    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 60       |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | -9.5     |\n",
      "| steps                   | 40523    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | -11.8    |\n",
      "| steps                   | 42804    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | -10.2    |\n",
      "| steps                   | 44924    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 53       |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | -11.6    |\n",
      "| steps                   | 47186    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 50       |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | -17.3    |\n",
      "| steps                   | 50015    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 48       |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | -15.5    |\n",
      "| steps                   | 52661    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 45       |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | -18.9    |\n",
      "| steps                   | 55651    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 42       |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | -23.8    |\n",
      "| steps                   | 59129    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 38       |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | -23.6    |\n",
      "| steps                   | 62584    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 35       |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | -22      |\n",
      "| steps                   | 65888    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 31       |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | -32.4    |\n",
      "| steps                   | 70230    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 26       |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | -38.9    |\n",
      "| steps                   | 75217    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 19       |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | -53.3    |\n",
      "| steps                   | 81646    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 11       |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | -71      |\n",
      "| steps                   | 89843    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | -128     |\n",
      "| steps                   | 103693   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | -731     |\n",
      "| steps                   | 177866   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 2         |\n",
      "| episodes                | 3100      |\n",
      "| mean 100 episode reward | -1.49e+03 |\n",
      "| steps                   | 327650    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 2         |\n",
      "| episodes                | 3200      |\n",
      "| mean 100 episode reward | -1.28e+03 |\n",
      "| steps                   | 456681    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | -1.1e+03 |\n",
      "| steps                   | 567398   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 2         |\n",
      "| episodes                | 3400      |\n",
      "| mean 100 episode reward | -1.08e+03 |\n",
      "| steps                   | 676658    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | -932     |\n",
      "| steps                   | 770940   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 2         |\n",
      "| episodes                | 3600      |\n",
      "| mean 100 episode reward | -1.05e+03 |\n",
      "| steps                   | 877293    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 2         |\n",
      "| episodes                | 3700      |\n",
      "| mean 100 episode reward | -1.01e+03 |\n",
      "| steps                   | 979556    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.deepq.dqn.DQN at 0x7f116a4b1b50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "\n",
    "model.save('ignore_simul2_'+timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -26.0\n"
     ]
    }
   ],
   "source": [
    "mean_reward, _ = ev.evaluate_random(gym.make('ctfsql-v0'), num_steps=100)\n",
    "print('Mean reward: {0}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -100.0\n"
     ]
    }
   ],
   "source": [
    "mean_reward, _ = ev.evaluate_model(model, env, num_steps=100)\n",
    "print('Mean reward: {0}'.format(mean_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
